Log file created at: 2017/08/11 15:32:57
Running on machine: CY-20170629JWYV
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0811 15:32:57.669248 11616 caffe.cpp:234] Using GPUs 0
I0811 15:32:57.987594 11616 caffe.cpp:239] GPU 0: GeForce GTX 1050
I0811 15:32:58.868937 11616 common.cpp:42] System entropy source not available, using fallback algorithm to generate seed instead.
I0811 15:32:58.868937 11616 solver.cpp:63] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "lenet"
solver_mode: GPU
device_id: 0
net: "lenet_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
I0811 15:32:58.885481 11616 solver.cpp:106] Creating training net from net file: lenet_train_test.prototxt
I0811 15:32:58.885982 11616 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0811 15:32:58.885982 11616 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0811 15:32:58.885982 11616 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0811 15:32:58.885982 11616 layer_factory.hpp:77] Creating layer mnist
I0811 15:32:58.886483 11616 common.cpp:42] System entropy source not available, using fallback algorithm to generate seed instead.
I0811 15:32:58.886483 11616 net.cpp:100] Creating Layer mnist
I0811 15:32:58.886483 11616 net.cpp:408] mnist -> data
I0811 15:32:58.886483 11616 net.cpp:408] mnist -> label
I0811 15:32:58.894003 11344 common.cpp:42] System entropy source not available, using fallback algorithm to generate seed instead.
I0811 15:32:58.894505 11344 db_lmdb.cpp:51] Opened lmdb mnist_train_lmdb
I0811 15:32:59.823974 11616 data_layer.cpp:41] output data size: 64,1,28,28
I0811 15:32:59.919729 11616 net.cpp:150] Setting up mnist
I0811 15:32:59.919729 11616 net.cpp:157] Top shape: 64 1 28 28 (50176)
I0811 15:32:59.919729 11616 net.cpp:157] Top shape: 64 (64)
I0811 15:32:59.919729 11616 net.cpp:165] Memory required for data: 200960
I0811 15:32:59.919729 11616 layer_factory.hpp:77] Creating layer conv1
I0811 15:32:59.919729 11616 net.cpp:100] Creating Layer conv1
I0811 15:32:59.919729 11616 net.cpp:434] conv1 <- data
I0811 15:32:59.920230 11616 net.cpp:408] conv1 -> conv1
I0811 15:33:00.054086 12552 common.cpp:42] System entropy source not available, using fallback algorithm to generate seed instead.
I0811 15:33:01.282852 11616 net.cpp:150] Setting up conv1
I0811 15:33:01.282852 11616 net.cpp:157] Top shape: 64 20 24 24 (737280)
I0811 15:33:01.282852 11616 net.cpp:165] Memory required for data: 3150080
I0811 15:33:01.282852 11616 layer_factory.hpp:77] Creating layer pool1
I0811 15:33:01.282852 11616 net.cpp:100] Creating Layer pool1
I0811 15:33:01.282852 11616 net.cpp:434] pool1 <- conv1
I0811 15:33:01.282852 11616 net.cpp:408] pool1 -> pool1
I0811 15:33:01.283354 11616 net.cpp:150] Setting up pool1
I0811 15:33:01.283354 11616 net.cpp:157] Top shape: 64 20 12 12 (184320)
I0811 15:33:01.283354 11616 net.cpp:165] Memory required for data: 3887360
I0811 15:33:01.283354 11616 layer_factory.hpp:77] Creating layer conv2
I0811 15:33:01.283354 11616 net.cpp:100] Creating Layer conv2
I0811 15:33:01.283354 11616 net.cpp:434] conv2 <- pool1
I0811 15:33:01.283354 11616 net.cpp:408] conv2 -> conv2
I0811 15:33:01.290874 11616 net.cpp:150] Setting up conv2
I0811 15:33:01.290874 11616 net.cpp:157] Top shape: 64 50 8 8 (204800)
I0811 15:33:01.290874 11616 net.cpp:165] Memory required for data: 4706560
I0811 15:33:01.291375 11616 layer_factory.hpp:77] Creating layer pool2
I0811 15:33:01.291375 11616 net.cpp:100] Creating Layer pool2
I0811 15:33:01.291375 11616 net.cpp:434] pool2 <- conv2
I0811 15:33:01.291375 11616 net.cpp:408] pool2 -> pool2
I0811 15:33:01.302906 11616 net.cpp:150] Setting up pool2
I0811 15:33:01.303407 11616 net.cpp:157] Top shape: 64 50 4 4 (51200)
I0811 15:33:01.303407 11616 net.cpp:165] Memory required for data: 4911360
I0811 15:33:01.303407 11616 layer_factory.hpp:77] Creating layer ip1
I0811 15:33:01.303407 11616 net.cpp:100] Creating Layer ip1
I0811 15:33:01.303407 11616 net.cpp:434] ip1 <- pool2
I0811 15:33:01.303407 11616 net.cpp:408] ip1 -> ip1
I0811 15:33:01.332485 11616 net.cpp:150] Setting up ip1
I0811 15:33:01.332485 11616 net.cpp:157] Top shape: 64 500 (32000)
I0811 15:33:01.332485 11616 net.cpp:165] Memory required for data: 5039360
I0811 15:33:01.332485 11616 layer_factory.hpp:77] Creating layer relu1
I0811 15:33:01.332485 11616 net.cpp:100] Creating Layer relu1
I0811 15:33:01.332485 11616 net.cpp:434] relu1 <- ip1
I0811 15:33:01.332485 11616 net.cpp:395] relu1 -> ip1 (in-place)
I0811 15:33:01.346521 11616 net.cpp:150] Setting up relu1
I0811 15:33:01.346521 11616 net.cpp:157] Top shape: 64 500 (32000)
I0811 15:33:01.346521 11616 net.cpp:165] Memory required for data: 5167360
I0811 15:33:01.347023 11616 layer_factory.hpp:77] Creating layer ip2
I0811 15:33:01.347023 11616 net.cpp:100] Creating Layer ip2
I0811 15:33:01.347023 11616 net.cpp:434] ip2 <- ip1
I0811 15:33:01.347023 11616 net.cpp:408] ip2 -> ip2
I0811 15:33:01.358554 11616 net.cpp:150] Setting up ip2
I0811 15:33:01.358554 11616 net.cpp:157] Top shape: 64 10 (640)
I0811 15:33:01.358554 11616 net.cpp:165] Memory required for data: 5169920
I0811 15:33:01.358554 11616 layer_factory.hpp:77] Creating layer loss
I0811 15:33:01.358554 11616 net.cpp:100] Creating Layer loss
I0811 15:33:01.358554 11616 net.cpp:434] loss <- ip2
I0811 15:33:01.358554 11616 net.cpp:434] loss <- label
I0811 15:33:01.358554 11616 net.cpp:408] loss -> loss
I0811 15:33:01.358554 11616 layer_factory.hpp:77] Creating layer loss
I0811 15:33:01.362565 11616 net.cpp:150] Setting up loss
I0811 15:33:01.362565 11616 net.cpp:157] Top shape: (1)
I0811 15:33:01.362565 11616 net.cpp:160]     with loss weight 1
I0811 15:33:01.362565 11616 net.cpp:165] Memory required for data: 5169924
I0811 15:33:01.362565 11616 net.cpp:226] loss needs backward computation.
I0811 15:33:01.362565 11616 net.cpp:226] ip2 needs backward computation.
I0811 15:33:01.362565 11616 net.cpp:226] relu1 needs backward computation.
I0811 15:33:01.362565 11616 net.cpp:226] ip1 needs backward computation.
I0811 15:33:01.362565 11616 net.cpp:226] pool2 needs backward computation.
I0811 15:33:01.362565 11616 net.cpp:226] conv2 needs backward computation.
I0811 15:33:01.362565 11616 net.cpp:226] pool1 needs backward computation.
I0811 15:33:01.362565 11616 net.cpp:226] conv1 needs backward computation.
I0811 15:33:01.362565 11616 net.cpp:228] mnist does not need backward computation.
I0811 15:33:01.362565 11616 net.cpp:270] This network produces output loss
I0811 15:33:01.362565 11616 net.cpp:283] Network initialization done.
I0811 15:33:01.363066 11616 solver.cpp:196] Creating test net (#0) specified by net file: lenet_train_test.prototxt
I0811 15:33:01.363066 11616 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0811 15:33:01.363066 11616 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0811 15:33:01.363066 11616 layer_factory.hpp:77] Creating layer mnist
I0811 15:33:01.363567 11616 net.cpp:100] Creating Layer mnist
I0811 15:33:01.363567 11616 net.cpp:408] mnist -> data
I0811 15:33:01.363567 11616 net.cpp:408] mnist -> label
I0811 15:33:01.413700 13004 common.cpp:42] System entropy source not available, using fallback algorithm to generate seed instead.
I0811 15:33:01.433253 13004 db_lmdb.cpp:51] Opened lmdb mnist_test_lmdb
I0811 15:33:01.433753 11616 data_layer.cpp:41] output data size: 100,1,28,28
I0811 15:33:01.441774 11616 net.cpp:150] Setting up mnist
I0811 15:33:01.441774 11616 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0811 15:33:01.441774 11616 net.cpp:157] Top shape: 100 (100)
I0811 15:33:01.441774 11616 net.cpp:165] Memory required for data: 314000
I0811 15:33:01.441774 11616 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0811 15:33:01.441774 11616 net.cpp:100] Creating Layer label_mnist_1_split
I0811 15:33:01.441774 11616 net.cpp:434] label_mnist_1_split <- label
I0811 15:33:01.441774 11616 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_0
I0811 15:33:01.441774 11616 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_1
I0811 15:33:01.441774 11616 net.cpp:150] Setting up label_mnist_1_split
I0811 15:33:01.441774 11616 net.cpp:157] Top shape: 100 (100)
I0811 15:33:01.441774 11616 net.cpp:157] Top shape: 100 (100)
I0811 15:33:01.441774 11616 net.cpp:165] Memory required for data: 314800
I0811 15:33:01.441774 11616 layer_factory.hpp:77] Creating layer conv1
I0811 15:33:01.441774 11616 net.cpp:100] Creating Layer conv1
I0811 15:33:01.441774 11616 net.cpp:434] conv1 <- data
I0811 15:33:01.441774 11616 net.cpp:408] conv1 -> conv1
I0811 15:33:01.448292 11616 net.cpp:150] Setting up conv1
I0811 15:33:01.448292 11616 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0811 15:33:01.448292 11616 net.cpp:165] Memory required for data: 4922800
I0811 15:33:01.448292 11616 layer_factory.hpp:77] Creating layer pool1
I0811 15:33:01.448292 11616 net.cpp:100] Creating Layer pool1
I0811 15:33:01.448292 11616 net.cpp:434] pool1 <- conv1
I0811 15:33:01.448292 11616 net.cpp:408] pool1 -> pool1
I0811 15:33:01.448292 11616 net.cpp:150] Setting up pool1
I0811 15:33:01.448292 11616 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0811 15:33:01.448793 11616 net.cpp:165] Memory required for data: 6074800
I0811 15:33:01.448793 11616 layer_factory.hpp:77] Creating layer conv2
I0811 15:33:01.448793 11616 net.cpp:100] Creating Layer conv2
I0811 15:33:01.448793 11616 net.cpp:434] conv2 <- pool1
I0811 15:33:01.448793 11616 net.cpp:408] conv2 -> conv2
I0811 15:33:01.450297 11616 net.cpp:150] Setting up conv2
I0811 15:33:01.450297 11616 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0811 15:33:01.450297 11616 net.cpp:165] Memory required for data: 7354800
I0811 15:33:01.450297 11616 layer_factory.hpp:77] Creating layer pool2
I0811 15:33:01.450297 11616 net.cpp:100] Creating Layer pool2
I0811 15:33:01.450297 11616 net.cpp:434] pool2 <- conv2
I0811 15:33:01.450297 11616 net.cpp:408] pool2 -> pool2
I0811 15:33:01.450799 11616 net.cpp:150] Setting up pool2
I0811 15:33:01.450799 11616 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0811 15:33:01.450799 11616 net.cpp:165] Memory required for data: 7674800
I0811 15:33:01.450799 11616 layer_factory.hpp:77] Creating layer ip1
I0811 15:33:01.450799 11616 net.cpp:100] Creating Layer ip1
I0811 15:33:01.450799 11616 net.cpp:434] ip1 <- pool2
I0811 15:33:01.450799 11616 net.cpp:408] ip1 -> ip1
I0811 15:33:01.453806 11616 net.cpp:150] Setting up ip1
I0811 15:33:01.453806 11616 net.cpp:157] Top shape: 100 500 (50000)
I0811 15:33:01.453806 11616 net.cpp:165] Memory required for data: 7874800
I0811 15:33:01.453806 11616 layer_factory.hpp:77] Creating layer relu1
I0811 15:33:01.453806 11616 net.cpp:100] Creating Layer relu1
I0811 15:33:01.453806 11616 net.cpp:434] relu1 <- ip1
I0811 15:33:01.453806 11616 net.cpp:395] relu1 -> ip1 (in-place)
I0811 15:33:01.454308 11616 net.cpp:150] Setting up relu1
I0811 15:33:01.454809 11616 net.cpp:157] Top shape: 100 500 (50000)
I0811 15:33:01.454809 11616 net.cpp:165] Memory required for data: 8074800
I0811 15:33:01.454809 11616 layer_factory.hpp:77] Creating layer ip2
I0811 15:33:01.454809 11616 net.cpp:100] Creating Layer ip2
I0811 15:33:01.454809 11616 net.cpp:434] ip2 <- ip1
I0811 15:33:01.454809 11616 net.cpp:408] ip2 -> ip2
I0811 15:33:01.454809 11616 net.cpp:150] Setting up ip2
I0811 15:33:01.454809 11616 net.cpp:157] Top shape: 100 10 (1000)
I0811 15:33:01.454809 11616 net.cpp:165] Memory required for data: 8078800
I0811 15:33:01.454809 11616 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0811 15:33:01.454809 11616 net.cpp:100] Creating Layer ip2_ip2_0_split
I0811 15:33:01.454809 11616 net.cpp:434] ip2_ip2_0_split <- ip2
I0811 15:33:01.454809 11616 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0811 15:33:01.454809 11616 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0811 15:33:01.454809 11616 net.cpp:150] Setting up ip2_ip2_0_split
I0811 15:33:01.454809 11616 net.cpp:157] Top shape: 100 10 (1000)
I0811 15:33:01.454809 11616 net.cpp:157] Top shape: 100 10 (1000)
I0811 15:33:01.454809 11616 net.cpp:165] Memory required for data: 8086800
I0811 15:33:01.454809 11616 layer_factory.hpp:77] Creating layer accuracy
I0811 15:33:01.454809 11616 net.cpp:100] Creating Layer accuracy
I0811 15:33:01.454809 11616 net.cpp:434] accuracy <- ip2_ip2_0_split_0
I0811 15:33:01.454809 11616 net.cpp:434] accuracy <- label_mnist_1_split_0
I0811 15:33:01.454809 11616 net.cpp:408] accuracy -> accuracy
I0811 15:33:01.454809 11616 net.cpp:150] Setting up accuracy
I0811 15:33:01.454809 11616 net.cpp:157] Top shape: (1)
I0811 15:33:01.454809 11616 net.cpp:165] Memory required for data: 8086804
I0811 15:33:01.454809 11616 layer_factory.hpp:77] Creating layer loss
I0811 15:33:01.455310 11616 net.cpp:100] Creating Layer loss
I0811 15:33:01.455310 11616 net.cpp:434] loss <- ip2_ip2_0_split_1
I0811 15:33:01.455310 11616 net.cpp:434] loss <- label_mnist_1_split_1
I0811 15:33:01.455310 11616 net.cpp:408] loss -> loss
I0811 15:33:01.455310 11616 layer_factory.hpp:77] Creating layer loss
I0811 15:33:01.457316 11616 net.cpp:150] Setting up loss
I0811 15:33:01.457316 11616 net.cpp:157] Top shape: (1)
I0811 15:33:01.457316 11616 net.cpp:160]     with loss weight 1
I0811 15:33:01.457316 11616 net.cpp:165] Memory required for data: 8086808
I0811 15:33:01.457316 11616 net.cpp:226] loss needs backward computation.
I0811 15:33:01.457316 11616 net.cpp:228] accuracy does not need backward computation.
I0811 15:33:01.457316 11616 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0811 15:33:01.457316 11616 net.cpp:226] ip2 needs backward computation.
I0811 15:33:01.457316 11616 net.cpp:226] relu1 needs backward computation.
I0811 15:33:01.457316 11616 net.cpp:226] ip1 needs backward computation.
I0811 15:33:01.457316 11616 net.cpp:226] pool2 needs backward computation.
I0811 15:33:01.457316 11616 net.cpp:226] conv2 needs backward computation.
I0811 15:33:01.457316 11616 net.cpp:226] pool1 needs backward computation.
I0811 15:33:01.457316 11616 net.cpp:226] conv1 needs backward computation.
I0811 15:33:01.457316 11616 net.cpp:228] label_mnist_1_split does not need backward computation.
I0811 15:33:01.457316 11616 net.cpp:228] mnist does not need backward computation.
I0811 15:33:01.457316 11616 net.cpp:270] This network produces output accuracy
I0811 15:33:01.457316 11616 net.cpp:270] This network produces output loss
I0811 15:33:01.457316 11616 net.cpp:283] Network initialization done.
I0811 15:33:01.457818 11616 solver.cpp:75] Solver scaffolding done.
I0811 15:33:01.457818 11616 caffe.cpp:268] Starting Optimization
I0811 15:33:01.457818 11616 solver.cpp:294] Solving LeNet
I0811 15:33:01.457818 11616 solver.cpp:295] Learning Rate Policy: inv
I0811 15:33:01.459322 11616 solver.cpp:358] Iteration 0, Testing net (#0)
I0811 15:33:01.461828 11616 blocking_queue.cpp:50] Data layer prefetch queue empty
I0811 15:33:01.498425 16060 common.cpp:42] System entropy source not available, using fallback algorithm to generate seed instead.
I0811 15:33:02.162691 11616 solver.cpp:425]     Test net output #0: accuracy = 0.1241
I0811 15:33:02.162691 11616 solver.cpp:425]     Test net output #1: loss = 2.38841 (* 1 = 2.38841 loss)
I0811 15:33:02.172216 11616 solver.cpp:243] Iteration 0, loss = 2.42416
I0811 15:33:02.172216 11616 solver.cpp:259]     Train net output #0: loss = 2.42416 (* 1 = 2.42416 loss)
I0811 15:33:02.172216 11616 sgd_solver.cpp:166] Iteration 0, lr = 0.01
I0811 15:33:03.521301 11616 solver.cpp:243] Iteration 100, loss = 0.198981
I0811 15:33:03.521301 11616 solver.cpp:259]     Train net output #0: loss = 0.198981 (* 1 = 0.198981 loss)
I0811 15:33:03.521301 11616 sgd_solver.cpp:166] Iteration 100, lr = 0.00992565
I0811 15:33:04.746057 11616 solver.cpp:243] Iteration 200, loss = 0.131934
I0811 15:33:04.746057 11616 solver.cpp:259]     Train net output #0: loss = 0.131934 (* 1 = 0.131934 loss)
I0811 15:33:04.746057 11616 sgd_solver.cpp:166] Iteration 200, lr = 0.00985258
I0811 15:33:05.958780 11616 solver.cpp:243] Iteration 300, loss = 0.169614
I0811 15:33:05.958780 11616 solver.cpp:259]     Train net output #0: loss = 0.169614 (* 1 = 0.169614 loss)
I0811 15:33:05.958780 11616 sgd_solver.cpp:166] Iteration 300, lr = 0.00978075
I0811 15:33:07.259237 11616 solver.cpp:243] Iteration 400, loss = 0.0769058
I0811 15:33:07.259738 11616 solver.cpp:259]     Train net output #0: loss = 0.0769058 (* 1 = 0.0769058 loss)
I0811 15:33:07.259738 11616 sgd_solver.cpp:166] Iteration 400, lr = 0.00971013
I0811 15:33:08.547660 11616 solver.cpp:358] Iteration 500, Testing net (#0)
I0811 15:33:09.295147 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9732
I0811 15:33:09.295147 11616 solver.cpp:425]     Test net output #1: loss = 0.0833868 (* 1 = 0.0833868 loss)
I0811 15:33:09.298656 11616 solver.cpp:243] Iteration 500, loss = 0.0847054
I0811 15:33:09.298656 11616 solver.cpp:259]     Train net output #0: loss = 0.0847054 (* 1 = 0.0847054 loss)
I0811 15:33:09.298656 11616 sgd_solver.cpp:166] Iteration 500, lr = 0.00964069
I0811 15:33:10.526921 11616 solver.cpp:243] Iteration 600, loss = 0.131253
I0811 15:33:10.526921 11616 solver.cpp:259]     Train net output #0: loss = 0.131253 (* 1 = 0.131253 loss)
I0811 15:33:10.526921 11616 sgd_solver.cpp:166] Iteration 600, lr = 0.0095724
I0811 15:33:11.642887 11616 solver.cpp:243] Iteration 700, loss = 0.113636
I0811 15:33:11.643388 11616 solver.cpp:259]     Train net output #0: loss = 0.113636 (* 1 = 0.113636 loss)
I0811 15:33:11.643388 11616 sgd_solver.cpp:166] Iteration 700, lr = 0.00950522
I0811 15:33:12.798458 11616 solver.cpp:243] Iteration 800, loss = 0.192717
I0811 15:33:12.798458 11616 solver.cpp:259]     Train net output #0: loss = 0.192717 (* 1 = 0.192717 loss)
I0811 15:33:12.798458 11616 sgd_solver.cpp:166] Iteration 800, lr = 0.00943913
I0811 15:33:14.028728 11616 solver.cpp:243] Iteration 900, loss = 0.183969
I0811 15:33:14.028728 11616 solver.cpp:259]     Train net output #0: loss = 0.183969 (* 1 = 0.183969 loss)
I0811 15:33:14.028728 11616 sgd_solver.cpp:166] Iteration 900, lr = 0.00937411
I0811 15:33:15.353749 11616 solver.cpp:358] Iteration 1000, Testing net (#0)
I0811 15:33:15.988939 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9821
I0811 15:33:15.988939 11616 solver.cpp:425]     Test net output #1: loss = 0.0553936 (* 1 = 0.0553936 loss)
I0811 15:33:15.991444 11616 solver.cpp:243] Iteration 1000, loss = 0.0816971
I0811 15:33:15.991444 11616 solver.cpp:259]     Train net output #0: loss = 0.0816973 (* 1 = 0.0816973 loss)
I0811 15:33:15.991946 11616 sgd_solver.cpp:166] Iteration 1000, lr = 0.00931012
I0811 15:33:17.257810 11616 solver.cpp:243] Iteration 1100, loss = 0.00738408
I0811 15:33:17.257810 11616 solver.cpp:259]     Train net output #0: loss = 0.00738424 (* 1 = 0.00738424 loss)
I0811 15:33:17.257810 11616 sgd_solver.cpp:166] Iteration 1100, lr = 0.00924715
I0811 15:33:18.471536 11616 solver.cpp:243] Iteration 1200, loss = 0.017018
I0811 15:33:18.471536 11616 solver.cpp:259]     Train net output #0: loss = 0.0170182 (* 1 = 0.0170182 loss)
I0811 15:33:18.471536 11616 sgd_solver.cpp:166] Iteration 1200, lr = 0.00918515
I0811 15:33:19.553411 11616 solver.cpp:243] Iteration 1300, loss = 0.0151278
I0811 15:33:19.553411 11616 solver.cpp:259]     Train net output #0: loss = 0.0151279 (* 1 = 0.0151279 loss)
I0811 15:33:19.553411 11616 sgd_solver.cpp:166] Iteration 1300, lr = 0.00912412
I0811 15:33:20.723521 11616 solver.cpp:243] Iteration 1400, loss = 0.00497258
I0811 15:33:20.724022 11616 solver.cpp:259]     Train net output #0: loss = 0.00497267 (* 1 = 0.00497267 loss)
I0811 15:33:20.724022 11616 sgd_solver.cpp:166] Iteration 1400, lr = 0.00906403
I0811 15:33:22.012447 11616 solver.cpp:358] Iteration 1500, Testing net (#0)
I0811 15:33:22.600008 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9849
I0811 15:33:22.600008 11616 solver.cpp:425]     Test net output #1: loss = 0.050007 (* 1 = 0.050007 loss)
I0811 15:33:22.604019 11616 solver.cpp:243] Iteration 1500, loss = 0.0972671
I0811 15:33:22.604521 11616 solver.cpp:259]     Train net output #0: loss = 0.0972672 (* 1 = 0.0972672 loss)
I0811 15:33:22.604521 11616 sgd_solver.cpp:166] Iteration 1500, lr = 0.00900485
I0811 15:33:23.819249 11616 solver.cpp:243] Iteration 1600, loss = 0.107453
I0811 15:33:23.819249 11616 solver.cpp:259]     Train net output #0: loss = 0.107453 (* 1 = 0.107453 loss)
I0811 15:33:23.819249 11616 sgd_solver.cpp:166] Iteration 1600, lr = 0.00894657
I0811 15:33:24.729670 11616 solver.cpp:243] Iteration 1700, loss = 0.0171688
I0811 15:33:24.729670 11616 solver.cpp:259]     Train net output #0: loss = 0.0171689 (* 1 = 0.0171689 loss)
I0811 15:33:24.729670 11616 sgd_solver.cpp:166] Iteration 1700, lr = 0.00888916
I0811 15:33:25.503726 11616 solver.cpp:243] Iteration 1800, loss = 0.0230607
I0811 15:33:25.503726 11616 solver.cpp:259]     Train net output #0: loss = 0.0230608 (* 1 = 0.0230608 loss)
I0811 15:33:25.503726 11616 sgd_solver.cpp:166] Iteration 1800, lr = 0.0088326
I0811 15:33:26.037645 11616 solver.cpp:243] Iteration 1900, loss = 0.123233
I0811 15:33:26.037645 11616 solver.cpp:259]     Train net output #0: loss = 0.123233 (* 1 = 0.123233 loss)
I0811 15:33:26.037645 11616 sgd_solver.cpp:166] Iteration 1900, lr = 0.00877687
I0811 15:33:26.493356 11616 solver.cpp:358] Iteration 2000, Testing net (#0)
I0811 15:33:26.661303 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9858
I0811 15:33:26.661303 11616 solver.cpp:425]     Test net output #1: loss = 0.0451159 (* 1 = 0.0451159 loss)
I0811 15:33:26.662806 11616 solver.cpp:243] Iteration 2000, loss = 0.00966362
I0811 15:33:26.662806 11616 solver.cpp:259]     Train net output #0: loss = 0.00966375 (* 1 = 0.00966375 loss)
I0811 15:33:26.662806 11616 sgd_solver.cpp:166] Iteration 2000, lr = 0.00872196
I0811 15:33:27.136065 11616 solver.cpp:243] Iteration 2100, loss = 0.0157142
I0811 15:33:27.136065 11616 solver.cpp:259]     Train net output #0: loss = 0.0157143 (* 1 = 0.0157143 loss)
I0811 15:33:27.136065 11616 sgd_solver.cpp:166] Iteration 2100, lr = 0.00866784
I0811 15:33:27.603307 11616 solver.cpp:243] Iteration 2200, loss = 0.0217449
I0811 15:33:27.603808 11616 solver.cpp:259]     Train net output #0: loss = 0.021745 (* 1 = 0.021745 loss)
I0811 15:33:27.603808 11616 sgd_solver.cpp:166] Iteration 2200, lr = 0.0086145
I0811 15:33:28.068042 11616 solver.cpp:243] Iteration 2300, loss = 0.120864
I0811 15:33:28.068042 11616 solver.cpp:259]     Train net output #0: loss = 0.120864 (* 1 = 0.120864 loss)
I0811 15:33:28.068042 11616 sgd_solver.cpp:166] Iteration 2300, lr = 0.00856192
I0811 15:33:28.532275 11616 solver.cpp:243] Iteration 2400, loss = 0.00678199
I0811 15:33:28.532275 11616 solver.cpp:259]     Train net output #0: loss = 0.00678209 (* 1 = 0.00678209 loss)
I0811 15:33:28.532275 11616 sgd_solver.cpp:166] Iteration 2400, lr = 0.00851008
I0811 15:33:28.983976 11616 solver.cpp:358] Iteration 2500, Testing net (#0)
I0811 15:33:29.161949 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9867
I0811 15:33:29.161949 11616 solver.cpp:425]     Test net output #1: loss = 0.0445178 (* 1 = 0.0445178 loss)
I0811 15:33:29.162951 11616 solver.cpp:243] Iteration 2500, loss = 0.0276005
I0811 15:33:29.163453 11616 solver.cpp:259]     Train net output #0: loss = 0.0276006 (* 1 = 0.0276006 loss)
I0811 15:33:29.163453 11616 sgd_solver.cpp:166] Iteration 2500, lr = 0.00845897
I0811 15:33:29.620667 11616 solver.cpp:243] Iteration 2600, loss = 0.0464208
I0811 15:33:29.621170 11616 solver.cpp:259]     Train net output #0: loss = 0.0464209 (* 1 = 0.0464209 loss)
I0811 15:33:29.621170 11616 sgd_solver.cpp:166] Iteration 2600, lr = 0.00840857
I0811 15:33:30.078384 11616 solver.cpp:243] Iteration 2700, loss = 0.0512422
I0811 15:33:30.078887 11616 solver.cpp:259]     Train net output #0: loss = 0.0512423 (* 1 = 0.0512423 loss)
I0811 15:33:30.078887 11616 sgd_solver.cpp:166] Iteration 2700, lr = 0.00835886
I0811 15:33:30.526576 11616 solver.cpp:243] Iteration 2800, loss = 0.00253952
I0811 15:33:30.526576 11616 solver.cpp:259]     Train net output #0: loss = 0.00253964 (* 1 = 0.00253964 loss)
I0811 15:33:30.526576 11616 sgd_solver.cpp:166] Iteration 2800, lr = 0.00830984
I0811 15:33:30.987301 11616 solver.cpp:243] Iteration 2900, loss = 0.0151018
I0811 15:33:30.987301 11616 solver.cpp:259]     Train net output #0: loss = 0.0151019 (* 1 = 0.0151019 loss)
I0811 15:33:30.987301 11616 sgd_solver.cpp:166] Iteration 2900, lr = 0.00826148
I0811 15:33:31.448526 11616 solver.cpp:358] Iteration 3000, Testing net (#0)
I0811 15:33:31.604941 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9887
I0811 15:33:31.604941 11616 solver.cpp:425]     Test net output #1: loss = 0.0370999 (* 1 = 0.0370999 loss)
I0811 15:33:31.606446 11616 solver.cpp:243] Iteration 3000, loss = 0.0120668
I0811 15:33:31.606446 11616 solver.cpp:259]     Train net output #0: loss = 0.0120669 (* 1 = 0.0120669 loss)
I0811 15:33:31.606446 11616 sgd_solver.cpp:166] Iteration 3000, lr = 0.00821377
I0811 15:33:32.055138 11616 solver.cpp:243] Iteration 3100, loss = 0.0342426
I0811 15:33:32.055138 11616 solver.cpp:259]     Train net output #0: loss = 0.0342426 (* 1 = 0.0342426 loss)
I0811 15:33:32.055138 11616 sgd_solver.cpp:166] Iteration 3100, lr = 0.0081667
I0811 15:33:32.512353 11616 solver.cpp:243] Iteration 3200, loss = 0.00473494
I0811 15:33:32.512353 11616 solver.cpp:259]     Train net output #0: loss = 0.00473502 (* 1 = 0.00473502 loss)
I0811 15:33:32.512353 11616 sgd_solver.cpp:166] Iteration 3200, lr = 0.00812025
I0811 15:33:32.957537 11616 solver.cpp:243] Iteration 3300, loss = 0.0231591
I0811 15:33:32.957537 11616 solver.cpp:259]     Train net output #0: loss = 0.0231592 (* 1 = 0.0231592 loss)
I0811 15:33:32.957537 11616 sgd_solver.cpp:166] Iteration 3300, lr = 0.00807442
I0811 15:33:33.430794 11616 solver.cpp:243] Iteration 3400, loss = 0.00838732
I0811 15:33:33.430794 11616 solver.cpp:259]     Train net output #0: loss = 0.00838744 (* 1 = 0.00838744 loss)
I0811 15:33:33.430794 11616 sgd_solver.cpp:166] Iteration 3400, lr = 0.00802918
I0811 15:33:33.889012 11616 solver.cpp:358] Iteration 3500, Testing net (#0)
I0811 15:33:34.044926 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9876
I0811 15:33:34.044926 11616 solver.cpp:425]     Test net output #1: loss = 0.0391857 (* 1 = 0.0391857 loss)
I0811 15:33:34.046430 11616 solver.cpp:243] Iteration 3500, loss = 0.00713458
I0811 15:33:34.046430 11616 solver.cpp:259]     Train net output #0: loss = 0.0071347 (* 1 = 0.0071347 loss)
I0811 15:33:34.046430 11616 sgd_solver.cpp:166] Iteration 3500, lr = 0.00798454
I0811 15:33:34.503648 11616 solver.cpp:243] Iteration 3600, loss = 0.0278576
I0811 15:33:34.503648 11616 solver.cpp:259]     Train net output #0: loss = 0.0278578 (* 1 = 0.0278578 loss)
I0811 15:33:34.503648 11616 sgd_solver.cpp:166] Iteration 3600, lr = 0.00794046
I0811 15:33:34.975901 11616 solver.cpp:243] Iteration 3700, loss = 0.0155489
I0811 15:33:34.975901 11616 solver.cpp:259]     Train net output #0: loss = 0.0155491 (* 1 = 0.0155491 loss)
I0811 15:33:34.975901 11616 sgd_solver.cpp:166] Iteration 3700, lr = 0.00789695
I0811 15:33:35.428103 11616 solver.cpp:243] Iteration 3800, loss = 0.0068431
I0811 15:33:35.428606 11616 solver.cpp:259]     Train net output #0: loss = 0.0068432 (* 1 = 0.0068432 loss)
I0811 15:33:35.428606 11616 sgd_solver.cpp:166] Iteration 3800, lr = 0.007854
I0811 15:33:35.904368 11616 solver.cpp:243] Iteration 3900, loss = 0.0618018
I0811 15:33:35.904368 11616 solver.cpp:259]     Train net output #0: loss = 0.0618019 (* 1 = 0.0618019 loss)
I0811 15:33:35.904368 11616 sgd_solver.cpp:166] Iteration 3900, lr = 0.00781158
I0811 15:33:36.374117 11616 solver.cpp:358] Iteration 4000, Testing net (#0)
I0811 15:33:36.535045 11616 solver.cpp:425]     Test net output #0: accuracy = 0.989
I0811 15:33:36.535045 11616 solver.cpp:425]     Test net output #1: loss = 0.0327225 (* 1 = 0.0327225 loss)
I0811 15:33:36.536548 11616 solver.cpp:243] Iteration 4000, loss = 0.0108562
I0811 15:33:36.536548 11616 solver.cpp:259]     Train net output #0: loss = 0.0108562 (* 1 = 0.0108562 loss)
I0811 15:33:36.536548 11616 sgd_solver.cpp:166] Iteration 4000, lr = 0.00776969
I0811 15:33:36.982734 11616 solver.cpp:243] Iteration 4100, loss = 0.0308555
I0811 15:33:36.982734 11616 solver.cpp:259]     Train net output #0: loss = 0.0308556 (* 1 = 0.0308556 loss)
I0811 15:33:36.982734 11616 sgd_solver.cpp:166] Iteration 4100, lr = 0.00772833
I0811 15:33:37.438446 11616 solver.cpp:243] Iteration 4200, loss = 0.00499983
I0811 15:33:37.438446 11616 solver.cpp:259]     Train net output #0: loss = 0.00499989 (* 1 = 0.00499989 loss)
I0811 15:33:37.438446 11616 sgd_solver.cpp:166] Iteration 4200, lr = 0.00768748
I0811 15:33:37.900674 11616 solver.cpp:243] Iteration 4300, loss = 0.0482286
I0811 15:33:37.900674 11616 solver.cpp:259]     Train net output #0: loss = 0.0482287 (* 1 = 0.0482287 loss)
I0811 15:33:37.900674 11616 sgd_solver.cpp:166] Iteration 4300, lr = 0.00764712
I0811 15:33:38.358892 11616 solver.cpp:243] Iteration 4400, loss = 0.0142727
I0811 15:33:38.358892 11616 solver.cpp:259]     Train net output #0: loss = 0.0142728 (* 1 = 0.0142728 loss)
I0811 15:33:38.358892 11616 sgd_solver.cpp:166] Iteration 4400, lr = 0.00760726
I0811 15:33:38.800065 11616 solver.cpp:358] Iteration 4500, Testing net (#0)
I0811 15:33:38.958487 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9886
I0811 15:33:38.958487 11616 solver.cpp:425]     Test net output #1: loss = 0.0358314 (* 1 = 0.0358314 loss)
I0811 15:33:38.960492 11616 solver.cpp:243] Iteration 4500, loss = 0.010828
I0811 15:33:38.960492 11616 solver.cpp:259]     Train net output #0: loss = 0.0108281 (* 1 = 0.0108281 loss)
I0811 15:33:38.960492 11616 sgd_solver.cpp:166] Iteration 4500, lr = 0.00756788
I0811 15:33:39.450794 11616 solver.cpp:243] Iteration 4600, loss = 0.0249402
I0811 15:33:39.450794 11616 solver.cpp:259]     Train net output #0: loss = 0.0249403 (* 1 = 0.0249403 loss)
I0811 15:33:39.450794 11616 sgd_solver.cpp:166] Iteration 4600, lr = 0.00752897
I0811 15:33:39.909013 11616 solver.cpp:243] Iteration 4700, loss = 0.00940595
I0811 15:33:39.909013 11616 solver.cpp:259]     Train net output #0: loss = 0.00940604 (* 1 = 0.00940604 loss)
I0811 15:33:39.909013 11616 sgd_solver.cpp:166] Iteration 4700, lr = 0.00749052
I0811 15:33:40.354696 11616 solver.cpp:243] Iteration 4800, loss = 0.0163728
I0811 15:33:40.354696 11616 solver.cpp:259]     Train net output #0: loss = 0.0163728 (* 1 = 0.0163728 loss)
I0811 15:33:40.354696 11616 sgd_solver.cpp:166] Iteration 4800, lr = 0.00745253
I0811 15:33:40.804392 11616 solver.cpp:243] Iteration 4900, loss = 0.00487871
I0811 15:33:40.804392 11616 solver.cpp:259]     Train net output #0: loss = 0.00487879 (* 1 = 0.00487879 loss)
I0811 15:33:40.804392 11616 sgd_solver.cpp:166] Iteration 4900, lr = 0.00741498
I0811 15:33:41.267626 11616 solver.cpp:596] Snapshotting to binary proto file lenet_iter_5000.caffemodel
I0811 15:33:41.281661 11616 sgd_solver.cpp:336] Snapshotting solver state to binary proto file lenet_iter_5000.solverstate
I0811 15:33:41.286172 11616 solver.cpp:358] Iteration 5000, Testing net (#0)
I0811 15:33:41.445096 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9897
I0811 15:33:41.445096 11616 solver.cpp:425]     Test net output #1: loss = 0.0310353 (* 1 = 0.0310353 loss)
I0811 15:33:41.452615 11616 solver.cpp:243] Iteration 5000, loss = 0.0299047
I0811 15:33:41.452615 11616 solver.cpp:259]     Train net output #0: loss = 0.0299048 (* 1 = 0.0299048 loss)
I0811 15:33:41.452615 11616 sgd_solver.cpp:166] Iteration 5000, lr = 0.00737788
I0811 15:33:41.895292 11616 solver.cpp:243] Iteration 5100, loss = 0.0187905
I0811 15:33:41.895292 11616 solver.cpp:259]     Train net output #0: loss = 0.0187906 (* 1 = 0.0187906 loss)
I0811 15:33:41.895292 11616 sgd_solver.cpp:166] Iteration 5100, lr = 0.0073412
I0811 15:33:42.359025 11616 solver.cpp:243] Iteration 5200, loss = 0.00984493
I0811 15:33:42.359025 11616 solver.cpp:259]     Train net output #0: loss = 0.00984501 (* 1 = 0.00984501 loss)
I0811 15:33:42.359025 11616 sgd_solver.cpp:166] Iteration 5200, lr = 0.00730495
I0811 15:33:42.802703 11616 solver.cpp:243] Iteration 5300, loss = 0.00145082
I0811 15:33:42.802703 11616 solver.cpp:259]     Train net output #0: loss = 0.0014509 (* 1 = 0.0014509 loss)
I0811 15:33:42.802703 11616 sgd_solver.cpp:166] Iteration 5300, lr = 0.00726911
I0811 15:33:43.257913 11616 solver.cpp:243] Iteration 5400, loss = 0.00773451
I0811 15:33:43.257913 11616 solver.cpp:259]     Train net output #0: loss = 0.0077346 (* 1 = 0.0077346 loss)
I0811 15:33:43.257913 11616 sgd_solver.cpp:166] Iteration 5400, lr = 0.00723368
I0811 15:33:43.714627 11616 solver.cpp:358] Iteration 5500, Testing net (#0)
I0811 15:33:43.890094 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9888
I0811 15:33:43.890094 11616 solver.cpp:425]     Test net output #1: loss = 0.0332834 (* 1 = 0.0332834 loss)
I0811 15:33:43.893101 11616 solver.cpp:243] Iteration 5500, loss = 0.00665437
I0811 15:33:43.893604 11616 solver.cpp:259]     Train net output #0: loss = 0.00665446 (* 1 = 0.00665446 loss)
I0811 15:33:43.893604 11616 sgd_solver.cpp:166] Iteration 5500, lr = 0.00719865
I0811 15:33:44.342295 11616 solver.cpp:243] Iteration 5600, loss = 0.000936306
I0811 15:33:44.342797 11616 solver.cpp:259]     Train net output #0: loss = 0.000936397 (* 1 = 0.000936397 loss)
I0811 15:33:44.342797 11616 sgd_solver.cpp:166] Iteration 5600, lr = 0.00716402
I0811 15:33:44.788481 11616 solver.cpp:243] Iteration 5700, loss = 0.0034132
I0811 15:33:44.788481 11616 solver.cpp:259]     Train net output #0: loss = 0.0034133 (* 1 = 0.0034133 loss)
I0811 15:33:44.788481 11616 sgd_solver.cpp:166] Iteration 5700, lr = 0.00712977
I0811 15:33:45.242688 11616 solver.cpp:243] Iteration 5800, loss = 0.0144933
I0811 15:33:45.242688 11616 solver.cpp:259]     Train net output #0: loss = 0.0144934 (* 1 = 0.0144934 loss)
I0811 15:33:45.242688 11616 sgd_solver.cpp:166] Iteration 5800, lr = 0.0070959
I0811 15:33:45.699401 11616 solver.cpp:243] Iteration 5900, loss = 0.00468067
I0811 15:33:45.699401 11616 solver.cpp:259]     Train net output #0: loss = 0.00468077 (* 1 = 0.00468077 loss)
I0811 15:33:45.699401 11616 sgd_solver.cpp:166] Iteration 5900, lr = 0.0070624
I0811 15:33:46.167145 11616 solver.cpp:358] Iteration 6000, Testing net (#0)
I0811 15:33:46.335091 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9901
I0811 15:33:46.335091 11616 solver.cpp:425]     Test net output #1: loss = 0.0299837 (* 1 = 0.0299837 loss)
I0811 15:33:46.336596 11616 solver.cpp:243] Iteration 6000, loss = 0.00493496
I0811 15:33:46.336596 11616 solver.cpp:259]     Train net output #0: loss = 0.00493507 (* 1 = 0.00493507 loss)
I0811 15:33:46.336596 11616 sgd_solver.cpp:166] Iteration 6000, lr = 0.00702927
I0811 15:33:46.792809 11616 solver.cpp:243] Iteration 6100, loss = 0.00283885
I0811 15:33:46.792809 11616 solver.cpp:259]     Train net output #0: loss = 0.00283896 (* 1 = 0.00283896 loss)
I0811 15:33:46.792809 11616 sgd_solver.cpp:166] Iteration 6100, lr = 0.0069965
I0811 15:33:47.244508 11616 solver.cpp:243] Iteration 6200, loss = 0.00599966
I0811 15:33:47.244508 11616 solver.cpp:259]     Train net output #0: loss = 0.00599975 (* 1 = 0.00599975 loss)
I0811 15:33:47.244508 11616 sgd_solver.cpp:166] Iteration 6200, lr = 0.00696408
I0811 15:33:47.691696 11616 solver.cpp:243] Iteration 6300, loss = 0.0104683
I0811 15:33:47.691696 11616 solver.cpp:259]     Train net output #0: loss = 0.0104684 (* 1 = 0.0104684 loss)
I0811 15:33:47.691696 11616 sgd_solver.cpp:166] Iteration 6300, lr = 0.00693201
I0811 15:33:48.152922 11616 solver.cpp:243] Iteration 6400, loss = 0.00490933
I0811 15:33:48.152922 11616 solver.cpp:259]     Train net output #0: loss = 0.00490943 (* 1 = 0.00490943 loss)
I0811 15:33:48.152922 11616 sgd_solver.cpp:166] Iteration 6400, lr = 0.00690029
I0811 15:33:48.593593 11616 solver.cpp:358] Iteration 6500, Testing net (#0)
I0811 15:33:48.750010 11616 solver.cpp:425]     Test net output #0: accuracy = 0.99
I0811 15:33:48.750010 11616 solver.cpp:425]     Test net output #1: loss = 0.0325547 (* 1 = 0.0325547 loss)
I0811 15:33:48.752015 11616 solver.cpp:243] Iteration 6500, loss = 0.00876424
I0811 15:33:48.752015 11616 solver.cpp:259]     Train net output #0: loss = 0.00876437 (* 1 = 0.00876437 loss)
I0811 15:33:48.752015 11616 sgd_solver.cpp:166] Iteration 6500, lr = 0.0068689
I0811 15:33:49.214745 11616 solver.cpp:243] Iteration 6600, loss = 0.0270651
I0811 15:33:49.215246 11616 solver.cpp:259]     Train net output #0: loss = 0.0270653 (* 1 = 0.0270653 loss)
I0811 15:33:49.215246 11616 sgd_solver.cpp:166] Iteration 6600, lr = 0.00683784
I0811 15:33:49.663437 11616 solver.cpp:243] Iteration 6700, loss = 0.00907598
I0811 15:33:49.663437 11616 solver.cpp:259]     Train net output #0: loss = 0.00907611 (* 1 = 0.00907611 loss)
I0811 15:33:49.663437 11616 sgd_solver.cpp:166] Iteration 6700, lr = 0.00680711
I0811 15:33:50.112131 11616 solver.cpp:243] Iteration 6800, loss = 0.00246619
I0811 15:33:50.112131 11616 solver.cpp:259]     Train net output #0: loss = 0.00246633 (* 1 = 0.00246633 loss)
I0811 15:33:50.112131 11616 sgd_solver.cpp:166] Iteration 6800, lr = 0.0067767
I0811 15:33:50.562326 11616 solver.cpp:243] Iteration 6900, loss = 0.00494359
I0811 15:33:50.562326 11616 solver.cpp:259]     Train net output #0: loss = 0.00494372 (* 1 = 0.00494372 loss)
I0811 15:33:50.562326 11616 sgd_solver.cpp:166] Iteration 6900, lr = 0.0067466
I0811 15:33:51.003499 11616 solver.cpp:358] Iteration 7000, Testing net (#0)
I0811 15:33:51.170944 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9893
I0811 15:33:51.170944 11616 solver.cpp:425]     Test net output #1: loss = 0.0308032 (* 1 = 0.0308032 loss)
I0811 15:33:51.172950 11616 solver.cpp:243] Iteration 7000, loss = 0.0106447
I0811 15:33:51.172950 11616 solver.cpp:259]     Train net output #0: loss = 0.0106449 (* 1 = 0.0106449 loss)
I0811 15:33:51.172950 11616 sgd_solver.cpp:166] Iteration 7000, lr = 0.00671681
I0811 15:33:51.635179 11616 solver.cpp:243] Iteration 7100, loss = 0.0125353
I0811 15:33:51.635179 11616 solver.cpp:259]     Train net output #0: loss = 0.0125355 (* 1 = 0.0125355 loss)
I0811 15:33:51.635179 11616 sgd_solver.cpp:166] Iteration 7100, lr = 0.00668733
I0811 15:33:52.084372 11616 solver.cpp:243] Iteration 7200, loss = 0.00419466
I0811 15:33:52.084372 11616 solver.cpp:259]     Train net output #0: loss = 0.00419479 (* 1 = 0.00419479 loss)
I0811 15:33:52.084372 11616 sgd_solver.cpp:166] Iteration 7200, lr = 0.00665815
I0811 15:33:52.545598 11616 solver.cpp:243] Iteration 7300, loss = 0.0209976
I0811 15:33:52.545598 11616 solver.cpp:259]     Train net output #0: loss = 0.0209977 (* 1 = 0.0209977 loss)
I0811 15:33:52.545598 11616 sgd_solver.cpp:166] Iteration 7300, lr = 0.00662927
I0811 15:33:53.003314 11616 solver.cpp:243] Iteration 7400, loss = 0.00397004
I0811 15:33:53.003314 11616 solver.cpp:259]     Train net output #0: loss = 0.00397017 (* 1 = 0.00397017 loss)
I0811 15:33:53.003314 11616 sgd_solver.cpp:166] Iteration 7400, lr = 0.00660067
I0811 15:33:53.462033 11616 solver.cpp:358] Iteration 7500, Testing net (#0)
I0811 15:33:53.620955 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9897
I0811 15:33:53.620955 11616 solver.cpp:425]     Test net output #1: loss = 0.0334014 (* 1 = 0.0334014 loss)
I0811 15:33:53.622459 11616 solver.cpp:243] Iteration 7500, loss = 0.0029067
I0811 15:33:53.622459 11616 solver.cpp:259]     Train net output #0: loss = 0.00290683 (* 1 = 0.00290683 loss)
I0811 15:33:53.622459 11616 sgd_solver.cpp:166] Iteration 7500, lr = 0.00657236
I0811 15:33:54.091205 11616 solver.cpp:243] Iteration 7600, loss = 0.00597322
I0811 15:33:54.091205 11616 solver.cpp:259]     Train net output #0: loss = 0.00597334 (* 1 = 0.00597334 loss)
I0811 15:33:54.091205 11616 sgd_solver.cpp:166] Iteration 7600, lr = 0.00654433
I0811 15:33:54.539897 11616 solver.cpp:243] Iteration 7700, loss = 0.026407
I0811 15:33:54.539897 11616 solver.cpp:259]     Train net output #0: loss = 0.0264072 (* 1 = 0.0264072 loss)
I0811 15:33:54.539897 11616 sgd_solver.cpp:166] Iteration 7700, lr = 0.00651658
I0811 15:33:54.997112 11616 solver.cpp:243] Iteration 7800, loss = 0.00369585
I0811 15:33:54.997614 11616 solver.cpp:259]     Train net output #0: loss = 0.00369598 (* 1 = 0.00369598 loss)
I0811 15:33:54.997614 11616 sgd_solver.cpp:166] Iteration 7800, lr = 0.00648911
I0811 15:33:55.470371 11616 solver.cpp:243] Iteration 7900, loss = 0.00689803
I0811 15:33:55.470371 11616 solver.cpp:259]     Train net output #0: loss = 0.00689816 (* 1 = 0.00689816 loss)
I0811 15:33:55.470371 11616 sgd_solver.cpp:166] Iteration 7900, lr = 0.0064619
I0811 15:33:55.929591 11616 solver.cpp:358] Iteration 8000, Testing net (#0)
I0811 15:33:56.097036 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9898
I0811 15:33:56.097036 11616 solver.cpp:425]     Test net output #1: loss = 0.0302314 (* 1 = 0.0302314 loss)
I0811 15:33:56.098541 11616 solver.cpp:243] Iteration 8000, loss = 0.00708818
I0811 15:33:56.098541 11616 solver.cpp:259]     Train net output #0: loss = 0.00708831 (* 1 = 0.00708831 loss)
I0811 15:33:56.098541 11616 sgd_solver.cpp:166] Iteration 8000, lr = 0.00643496
I0811 15:33:56.553750 11616 solver.cpp:243] Iteration 8100, loss = 0.0246015
I0811 15:33:56.553750 11616 solver.cpp:259]     Train net output #0: loss = 0.0246016 (* 1 = 0.0246016 loss)
I0811 15:33:56.553750 11616 sgd_solver.cpp:166] Iteration 8100, lr = 0.00640827
I0811 15:33:57.007957 11616 solver.cpp:243] Iteration 8200, loss = 0.00824258
I0811 15:33:57.007957 11616 solver.cpp:259]     Train net output #0: loss = 0.00824273 (* 1 = 0.00824273 loss)
I0811 15:33:57.007957 11616 sgd_solver.cpp:166] Iteration 8200, lr = 0.00638185
I0811 15:33:57.471690 11616 solver.cpp:243] Iteration 8300, loss = 0.0381969
I0811 15:33:57.471690 11616 solver.cpp:259]     Train net output #0: loss = 0.038197 (* 1 = 0.038197 loss)
I0811 15:33:57.471690 11616 sgd_solver.cpp:166] Iteration 8300, lr = 0.00635568
I0811 15:33:57.920383 11616 solver.cpp:243] Iteration 8400, loss = 0.00798255
I0811 15:33:57.920383 11616 solver.cpp:259]     Train net output #0: loss = 0.0079827 (* 1 = 0.0079827 loss)
I0811 15:33:57.920383 11616 sgd_solver.cpp:166] Iteration 8400, lr = 0.00632975
I0811 15:33:58.363560 11616 solver.cpp:358] Iteration 8500, Testing net (#0)
I0811 15:33:58.526994 11616 solver.cpp:425]     Test net output #0: accuracy = 0.991
I0811 15:33:58.527496 11616 solver.cpp:425]     Test net output #1: loss = 0.0291402 (* 1 = 0.0291402 loss)
I0811 15:33:58.528498 11616 solver.cpp:243] Iteration 8500, loss = 0.0049576
I0811 15:33:58.528498 11616 solver.cpp:259]     Train net output #0: loss = 0.00495775 (* 1 = 0.00495775 loss)
I0811 15:33:58.528498 11616 sgd_solver.cpp:166] Iteration 8500, lr = 0.00630407
I0811 15:33:58.985713 11616 solver.cpp:243] Iteration 8600, loss = 0.000607285
I0811 15:33:58.985713 11616 solver.cpp:259]     Train net output #0: loss = 0.000607438 (* 1 = 0.000607438 loss)
I0811 15:33:58.985713 11616 sgd_solver.cpp:166] Iteration 8600, lr = 0.00627864
I0811 15:33:59.438920 11616 solver.cpp:243] Iteration 8700, loss = 0.00328761
I0811 15:33:59.438920 11616 solver.cpp:259]     Train net output #0: loss = 0.00328777 (* 1 = 0.00328777 loss)
I0811 15:33:59.438920 11616 sgd_solver.cpp:166] Iteration 8700, lr = 0.00625344
I0811 15:33:59.886608 11616 solver.cpp:243] Iteration 8800, loss = 0.00140834
I0811 15:33:59.886608 11616 solver.cpp:259]     Train net output #0: loss = 0.00140849 (* 1 = 0.00140849 loss)
I0811 15:33:59.886608 11616 sgd_solver.cpp:166] Iteration 8800, lr = 0.00622847
I0811 15:34:00.346832 11616 solver.cpp:243] Iteration 8900, loss = 0.000928415
I0811 15:34:00.346832 11616 solver.cpp:259]     Train net output #0: loss = 0.000928572 (* 1 = 0.000928572 loss)
I0811 15:34:00.346832 11616 sgd_solver.cpp:166] Iteration 8900, lr = 0.00620374
I0811 15:34:00.787503 11616 solver.cpp:358] Iteration 9000, Testing net (#0)
I0811 15:34:00.956451 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9903
I0811 15:34:00.956451 11616 solver.cpp:425]     Test net output #1: loss = 0.0303087 (* 1 = 0.0303087 loss)
I0811 15:34:00.958457 11616 solver.cpp:243] Iteration 9000, loss = 0.0160064
I0811 15:34:00.958457 11616 solver.cpp:259]     Train net output #0: loss = 0.0160065 (* 1 = 0.0160065 loss)
I0811 15:34:00.958457 11616 sgd_solver.cpp:166] Iteration 9000, lr = 0.00617924
I0811 15:34:01.408151 11616 solver.cpp:243] Iteration 9100, loss = 0.00851915
I0811 15:34:01.408151 11616 solver.cpp:259]     Train net output #0: loss = 0.00851931 (* 1 = 0.00851931 loss)
I0811 15:34:01.408151 11616 sgd_solver.cpp:166] Iteration 9100, lr = 0.00615496
I0811 15:34:01.876396 11616 solver.cpp:243] Iteration 9200, loss = 0.00142023
I0811 15:34:01.876396 11616 solver.cpp:259]     Train net output #0: loss = 0.00142038 (* 1 = 0.00142038 loss)
I0811 15:34:01.876396 11616 sgd_solver.cpp:166] Iteration 9200, lr = 0.0061309
I0811 15:34:02.337622 11616 solver.cpp:243] Iteration 9300, loss = 0.00738111
I0811 15:34:02.337622 11616 solver.cpp:259]     Train net output #0: loss = 0.00738126 (* 1 = 0.00738126 loss)
I0811 15:34:02.337622 11616 sgd_solver.cpp:166] Iteration 9300, lr = 0.00610706
I0811 15:34:02.801355 11616 solver.cpp:243] Iteration 9400, loss = 0.0185891
I0811 15:34:02.801355 11616 solver.cpp:259]     Train net output #0: loss = 0.0185893 (* 1 = 0.0185893 loss)
I0811 15:34:02.801355 11616 sgd_solver.cpp:166] Iteration 9400, lr = 0.00608343
I0811 15:34:03.261579 11616 solver.cpp:358] Iteration 9500, Testing net (#0)
I0811 15:34:03.443464 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9884
I0811 15:34:03.443464 11616 solver.cpp:425]     Test net output #1: loss = 0.0371829 (* 1 = 0.0371829 loss)
I0811 15:34:03.445971 11616 solver.cpp:243] Iteration 9500, loss = 0.00405504
I0811 15:34:03.445971 11616 solver.cpp:259]     Train net output #0: loss = 0.00405518 (* 1 = 0.00405518 loss)
I0811 15:34:03.445971 11616 sgd_solver.cpp:166] Iteration 9500, lr = 0.00606002
I0811 15:34:03.910205 11616 solver.cpp:243] Iteration 9600, loss = 0.00418962
I0811 15:34:03.910205 11616 solver.cpp:259]     Train net output #0: loss = 0.00418976 (* 1 = 0.00418976 loss)
I0811 15:34:03.910205 11616 sgd_solver.cpp:166] Iteration 9600, lr = 0.00603682
I0811 15:34:04.365916 11616 solver.cpp:243] Iteration 9700, loss = 0.0031132
I0811 15:34:04.365916 11616 solver.cpp:259]     Train net output #0: loss = 0.00311334 (* 1 = 0.00311334 loss)
I0811 15:34:04.365916 11616 sgd_solver.cpp:166] Iteration 9700, lr = 0.00601382
I0811 15:34:04.815611 11616 solver.cpp:243] Iteration 9800, loss = 0.00933946
I0811 15:34:04.815611 11616 solver.cpp:259]     Train net output #0: loss = 0.0093396 (* 1 = 0.0093396 loss)
I0811 15:34:04.815611 11616 sgd_solver.cpp:166] Iteration 9800, lr = 0.00599102
I0811 15:34:05.287365 11616 solver.cpp:243] Iteration 9900, loss = 0.00492432
I0811 15:34:05.287365 11616 solver.cpp:259]     Train net output #0: loss = 0.00492446 (* 1 = 0.00492446 loss)
I0811 15:34:05.287365 11616 sgd_solver.cpp:166] Iteration 9900, lr = 0.00596843
I0811 15:34:05.732548 11616 solver.cpp:596] Snapshotting to binary proto file lenet_iter_10000.caffemodel
I0811 15:34:05.746587 11616 sgd_solver.cpp:336] Snapshotting solver state to binary proto file lenet_iter_10000.solverstate
I0811 15:34:05.753605 11616 solver.cpp:332] Iteration 10000, loss = 0.0023173
I0811 15:34:05.753605 11616 solver.cpp:358] Iteration 10000, Testing net (#0)
I0811 15:34:05.909519 11616 solver.cpp:425]     Test net output #0: accuracy = 0.9908
I0811 15:34:05.909519 11616 solver.cpp:425]     Test net output #1: loss = 0.0288969 (* 1 = 0.0288969 loss)
I0811 15:34:05.909519 11616 solver.cpp:337] Optimization Done.
I0811 15:34:05.909519 11616 caffe.cpp:271] Optimization Done.
